# -*- coding: utf-8 -*-
"""2_Data_cleaning_en.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_my5dhvUtOVPB4FsMCNKU7w3is6rkuIh

# Emoji Sentiment Analysis with Tweets
        
## step2-Data cleaning
### 2.1-First EDA
### 2.2-Data cleaning
        
    1. remove url
    2. remove user names
    3. remove punctuations
    4. remove stopwords
    5. lower the words
    6. lemmatization
    7. keep only english characters and emojis

### 2.3-Extract pure emojis and text   
And, we generate 3 new coloumns which:
- remove all emojis
        
----
- include all emojis
        
----
- only include concerned emojis

![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)

### 2.1 Exploratory Data Analysis

Our dataset consists of Tweets which contain at least one of the following emoji
The baseline selection of common-used emoji refers to [Ian D. Wood, Sebastian Ruder, 2016](https://www.researchgate.net/publication/321057905_Emoji_as_Emotion_Tags_for_Tweets)
        
The tag of emoji is besed on human intuition accroding to the original paper, instead of meaning in context.
![image.png](attachment:image.png)
"""

# required libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from collections import  Counter
plt.style.use('ggplot')
stop=set(stopwords.words('english'))
import re
from nltk.tokenize import word_tokenize
import gensim
import string

# emoji lists

joy = ['\U0001F600', '\U0001F602', '\U0001F603', '\U0001F604',
          '\U0001F606', '\U0001F607', '\U0001F609', '\U0001F60A',
          '\U0001F60B', '\U0001F60C', '\U0001F60D', '\U0001F60E',
          '\U0001F60F', '\U0001F31E', '\U0000263A', '\U0001F618',
          '\U0001F61C', '\U0001F61D', '\U0001F61B', '\U0001F63A',
          '\U0001F638', '\U0001F639', '\U0001F63B', '\U0001F63C',
          '\U00002764', '\U0001F496', '\U0001F495', '\U0001F601',
          '\U00002665']#joy

anger = ['\U0001F62C', '\U0001F620', '\U0001F610',
          '\U0001F611', '\U0001F620', '\U0001F621', '\U0001F616',
          '\U0001F624', '\U0001F63E']#anger
fear = ['\U0001F605', '\U0001F626', '\U0001F627', '\U0001F631',
          '\U0001F628', '\U0001F630', '\U0001F640']#fear
sad = ['\U0001F614', '\U0001F615', '\U00002639', '\U0001F62B',
          '\U0001F629', '\U0001F622', '\U0001F625', '\U0001F62A',
          '\U0001F613', '\U0001F62D', '\U0001F63F', '\U0001F494']#sad

emojis = {'joy':joy, 'anger':anger,  'fear':fear, 'sad':sad}
for i in emojis:
    print('There are {} emoji contains in the cluster {}'.format(len(emojis[i]), i))
    print(i,emojis[i])



#load the crawed data
tweet = pd.read_csv('tweets3.csv')
# tweet = tweet[tweet.astype != 'Affect Dimension']#due to mistake when scraping, some rows are written as header repeatedly by mistake
tweet = tweet.drop_duplicates()#drop duplicated rows

# 2.1.1 data clusters contained in our dataset
# we considered the number of emoji in each cluster, as well as the balance of samples
sns.set(style="whitegrid", color_codes=True)
data = tweet.groupby("Affect Dimension").size().sort_values(ascending = False)
pal = sns.color_palette("Blues_d", len(data))
sns.barplot(x=data.index, y=data, palette=pal)

plt.show()

# 2.1.2 number of words in tweets
lengths = [len(t.split(' ')) for t in tweet['Tweet']]

plt.figure(figsize=(10,4))
plt.grid(axis = 'x')
plt.hist(lengths, bins = len(set(lengths)))
plt.show()

# most tweets contain words from 0 to 40
# Intuitively, this dosen't mean the longer tweets are outliers, we don't drop them so far
# but when taining models in the next section, we should consider the length distribution

# 2.1.3 find the common stopwords and punctuations in tweets

corpus=[]
for x in tweet['Tweet'].str.split():
    for i in x:
        corpus.append(i)

dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]
pal = sns.color_palette("Blues_d", len(top))
x,y=zip(*top)
plt.figure(figsize=(15,4))
plt.grid(axis = 'x')
plt.bar(x,y,color=pal)
plt.show()

# punctuations
dic=defaultdict(int)
import string
special = string.punctuation
for i in (corpus):
    if i in special:
        dic[i]+=1

x,y=zip(*dic.items())
pal = sns.color_palette("Blues_d", len(dic.items()))
plt.figure(figsize=(15,4))
plt.grid(axis = 'x')
plt.bar(x,y,color=pal)
plt.show()

"""### 2.2 Data Cleaning
We include 6 data cleaning steps:
        
    1. remove url
    2. remove user names
    3. remove punctuations
    4. remove stopwords
    5. lower the words
    6. lemmatization

Then, we construct a new column which remove all emoji for the future modeling
"""

################

# 2.2.1 data cleaning
# a new dataframe, keep the original dataset
df = pd.read_csv('tweets3.csv')
# df = df[df.type != 'Affect Dimension']

# i. remove url
def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',text)



df['Tweet']=df['Tweet'].apply(lambda x : remove_URL(x))

# ii. remove user names
def remove_name(text):
    name=re.compile(r'R?T?.*?@\S+')
    return name.sub(r'',text)



df['Tweet']=df['Tweet'].apply(lambda x : remove_name(x))

# iii. remove punctuations
def remove_punct(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)


df['Tweet']=df['Tweet'].apply(lambda x : remove_punct(x))

# iv. remove stopwords
from gensim.parsing.preprocessing import remove_stopwords
example = df['Tweet'][4]


df['Tweet']=df['Tweet'].apply(lambda x : remove_stopwords(x))



df['Tweet']=df['Tweet'].apply(lambda x : x.lower())

#v.i. lemmatization

from textblob import TextBlob

def lemm(text):
    textTB = TextBlob(text)
    words = textTB.words
    words_lemmatized = words.lemmatize()
    return ' '.join(words_lemmatized)



# Commented out IPython magic to ensure Python compatibility.
# %time df['tweets']=df['tweets'].apply(lambda x : lemm(x))

# v.ii. keep only english characters and emojis

import emoji
import nltk
nltk.download('words')
words = set(nltk.corpus.words.words())

def keepengemoji(text):
    ls = []
    for w in text.split(' '):
        if w in words:
            ls.append(w)
        elif w in emoji.EMOJI_DATA.keys():
            w = ' '+w+' '
            ls.append(w)
        else:
            continue
    return ' '.join(ls)



df['Tweet']=df['Tweet'].apply(lambda x : keepengemoji(x))

"""### 2.3. Extract pure text and emojis"""

################
# i. Form a new column (pure text)

# function of removing emoji
import emoji

def remove_emojis(text):
  return ''.join(c for c in text if c not in emoji.EMOJI_DATA.keys())



puretext = [remove_emojis(t) for t in df['Tweet']]
puretext[:10]

# function of extracting emoji
import emoji

def extract_emojis(text):
  return ' '.join(c for c in text if c in emoji.EMOJI_DATA.keys())



emojis = [extract_emojis(t) for t in df['Tweet']]
emojis[:10]

# function of get concerned emojis
allemoji = joy + anger  + fear + sad

def concern_emojis(text):
  return ' '.join(c for c in text if c in allemoji)



c_emojis = [concern_emojis(t) for t in df['Tweet']]
c_emojis[:10]

# a new df
df2 = df
df2['puretext'] = puretext
df2['emojis'] = emojis
df2['c_emojis'] = c_emojis
df2[['Tweet','puretext','emojis','c_emojis','Affect Dimension']]

# # # some emojis in tweets are removed due to text processing
# df2.query('emojis == ""')



# we need to drop these invalid samples
df3 = df2[df2.emojis != '']
df3[['Tweet','puretext','emojis','Affect Dimension']]

df3[['Tweet','puretext','emojis','c_emojis']]

###
# split multi-emojis in tweets
import emoji
def emosplit(text):
    words = []
    for c in text:
        if c in emoji.EMOJI_DATA.keys():
            c = ' '+c+' '
            words.append(c)
        else:
            words.append(c)
    return ''.join(words)



stext = [emosplit(t) for t in df3['Tweet']]
stext[:10]

df3['stweets'] = stext
df3[['stweets','Tweet','puretext','emojis','c_emojis','Affect Dimension']]

################
# 2.2.3 Look at the parsed dataset

sns.set(style="whitegrid", color_codes=True)
data = df3.groupby("Affect Dimension").size().sort_values(ascending = False)
pal = sns.color_palette("Blues_d", len(data))
sns.barplot(x=data.index, y=data, palette=pal)

plt.show()


# number of words in tweets
lengths = [len(t.split(' ')) for t in df3['Tweet']]

plt.figure(figsize=(10,4))
plt.grid(axis = 'x')
plt.hist(lengths, bins = len(set(lengths)))
plt.show()

# after data cleaning,
# most tweets contain words from 0 to 25

# save new csvs
df3.to_csv('tweets_cleaned_drop.csv')

# upload to github version
df4 = df3[['Tweet','stweets','puretext','emojis','c_emojis','Affect Dimension']]
df4.to_csv('data_tweets_cleaned_en.csv')



