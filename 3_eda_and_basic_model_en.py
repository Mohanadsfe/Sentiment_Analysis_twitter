# -*- coding: utf-8 -*-
"""3_EDA_and_basic_model_en.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b0TT0sNuBd837ZXywaUnQN0eA8a-Rxgn

# Emoji Sentiment Analysis with Tweets
        
## step3-EDA & Basic models
### 3.1 data exploring
1. plot the most requent emoji

### 3.2 topic models
1. topic moedel with all emoji --- first galance of emoji meaning
2. topic model with only concerned emojis

### 3.3 word embedding
1. word embedding with only emoji
2. word embedding with both emoji and words

![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)
"""

# ! git clone https://github.com/Liagogo/Twitter-Sentiment-Analysis-Emoji-Embedding-and-LSTM

# required libraries
import pandas as pd
import matplotlib.pyplot as plt

import seaborn as sns
import numpy as np

from nltk.corpus import stopwords
from nltk.util import ngrams
from sklearn.feature_extraction.text import CountVectorizer
from collections import defaultdict
from collections import  Counter

plt.style.use('ggplot')
stop=set(stopwords.words('english'))

import re
from nltk.tokenize import word_tokenize
import gensim
import string

# emoji lists

joy = ['\U0001F600', '\U0001F602', '\U0001F603', '\U0001F604',
          '\U0001F606', '\U0001F607', '\U0001F609', '\U0001F60A',
          '\U0001F60B', '\U0001F60C', '\U0001F60D', '\U0001F60E',
          '\U0001F60F', '\U0001F31E', '\U0000263A', '\U0001F618',
          '\U0001F61C', '\U0001F61D', '\U0001F61B', '\U0001F63A',
          '\U0001F638', '\U0001F639', '\U0001F63B', '\U0001F63C',
          '\U00002764', '\U0001F496', '\U0001F495', '\U0001F601',
          '\U00002665']#joy

anger = ['\U0001F62C', '\U0001F620', '\U0001F610',
          '\U0001F611', '\U0001F620', '\U0001F621', '\U0001F616',
          '\U0001F624', '\U0001F63E']#anger
fear = ['\U0001F605', '\U0001F626', '\U0001F627', '\U0001F631',
          '\U0001F628', '\U0001F630', '\U0001F640']#fear
sad = ['\U0001F614', '\U0001F615', '\U00002639', '\U0001F62B',
          '\U0001F629', '\U0001F622', '\U0001F625', '\U0001F62A',
          '\U0001F613', '\U0001F62D', '\U0001F63F', '\U0001F494']#sad

emojilist = {'joy':joy, 'anger':anger, 'fear':fear, 'sad':sad}
for i in emojilist:
    print('There are {} emoji contains in the cluster {}'.format(len(emojilist[i]), i))
    print(i,emojilist[i])

allemoji = allemoji = joy + anger + fear + sad

##########
#3.1 Cleaned Data Exploring
#load the cleaned data
df = pd.read_csv('tweets_cleaned_drop.csv')

df[['Tweet','stweets','emojis','c_emojis','Affect Dimension']]



#### all emojis frequencies
#get the emoji list
wordslist = []
for t in df['emojis']:
    t = t.split()
    wordslist += t

wordslist[:3]

x = np.array(wordslist)
uniquewordslist = np.unique(x)#all unique emojis shown in the dataset
#example
uniquewordslist[100]

ls = []
count = Counter(wordslist)
allemoji = joy + anger  + fear + sad 
for i in allemoji:
    x,y = i,count[i]
    ls.append([x,y])
# x,y = zip(*ls)
# x,y

ls = sorted(ls)
import pandas as pd
fredf = pd.DataFrame(ls,columns=['emoji','count'])
fredf.sort_values(by=['count'],ascending=False, inplace=True)
fredf[:11]



fredf.to_csv('fre_emojis_en.csv')

# draw the most frequently used emoji:
# from matplotlib.font_manager import FontProperties
# prop = FontProperties(fname='/Library/Fonts/Euclid Fraktur.dfont')

freqs = fredf['count'][:11]
labels = fredf['emoji'][:11]
# Plot the figure
plt.figure(figsize=(15, 4))
plt.rcParams['font.family'] = 'sans-serif'  # Use a generic sans-serif font

# plt.rcParams['font.family'] = prop.get_family()
# plt.grid(axis = 'x')
# plt.bar(labels,freqs,color='pink')
# plt.show()


ax = pd.Series(freqs).plot(kind='bar', color='lightblue', width=0.8)
ax.set_title('Most Frequently Used Emojis')
ax.set_ylabel('Frequency', fontsize=15)
ax.set_xlabel('Emojis', fontsize=15)
ax.set_facecolor(color='white')
ax.grid(False)
plt.tick_params(
    axis='x',
    which='both',
    bottom=False,
    top=False,
    labelbottom=False,
)

new_ylim = ax.get_ylim()[1]+30
ax.set_ylim((0, new_ylim))

rects = ax.patches

# Make labels
# for rect, label in zip(rects, labels):
#     height = rect.get_height()
#     plt.annotate(
#         label,
#         (rect.get_x() + rect.get_width()/2, height+5),
#         ha="center",
#         va="bottom",
#         fontsize=30,
#         fontproperties=prop.get_name()
#     )

# plt.show()

"""## 3.2 Topic model
1. topic moedel with all emoji --- first galance of emoji meaning
2. topic model with only concerned emojis
"""

######################
##########
# 3.2 Topic model with emojis
# prepare emoji list for topic model
emlist = [em.split() for em in df['emojis']]
# emlist[:10]

# Commented out IPython magic to ensure Python compatibility.
# train LDA model
from gensim.parsing.preprocessing import remove_stopwords
from gensim import corpora
from gensim import models

# dictionary and corpus
vocabs = corpora.Dictionary(emlist)
corpus = [vocabs.doc2bow(item) for item in emlist]

#lda model
lda = models.ldamodel.LdaModel(corpus=corpus, id2word=vocabs, num_topics=9)
print(lda.show_topics(-1))
corpus_lda = lda[corpus]
print(corpus_lda[0])

# Commented out IPython magic to ensure Python compatibility.
#vidualization
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import warnings
warnings.filterwarnings('ignore')#ignore the warning

vis_data = gensimvis.prepare(lda, corpus, vocabs, mds='mmds')
pyLDAvis.display(vis_data)

# get the top 10 emoji related with each topic
my_dict = {'Topic_' + str(i): [token for token, score in lda.show_topic(i, topn=10)] for i in range(0, lda.num_topics)}
for i in range(9):
    key = 'Topic_' + str(i)
    print('Top 10 related emoji in', key)
    print(my_dict[key])

###
#only explore our concerned emojis

# prepare emoji list for topic model
cels = []
emlist = [em.split() for em in df['emojis']]
for i in emlist:
    e = []
    for emo in i:
        if emo in allemoji:
            e.append(emo)
    cels.append(e)

# print(emlist[:10])
# print(cels[:10])


# Commented out IPython magic to ensure Python compatibility.

# train LDA model
from gensim.parsing.preprocessing import remove_stopwords
from gensim import corpora
from gensim import models

# dictionary and corpus
vocabs = corpora.Dictionary(emlist)
corpus = [vocabs.doc2bow(item) for item in emlist]

#lda model
# %time lda = models.ldamodel.LdaModel(corpus=corpus, id2word=vocabs, num_topics=6)
print(lda.show_topics(-1))
corpus_lda = lda[corpus]
print(corpus_lda[0])

# Commented out IPython magic to ensure Python compatibility.
#vidualization
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import warnings
warnings.filterwarnings('ignore')#ignore the warning

# %time vis_data = gensimvis.prepare(lda, corpus, vocabs, mds='mmds')
pyLDAvis.display(vis_data)

# get the top 10 emoji related with each topic
my_dict = {'Topic_' + str(i): [token for token, score in lda.show_topic(i, topn=10)] for i in range(0, lda.num_topics)}
for i in range(6):
    key = 'Topic_' + str(i)
    print('Top 10 related emoji in', key)
    print(my_dict[key])

# e-LDA
# select stable topics
from gensim.models import EnsembleLda
emlist = cels
# dictionary and corpus
vocabs = corpora.Dictionary(emlist)
corpus = [vocabs.doc2bow(item) for item in emlist]

# Ensemble LDA model
# elda = EnsembleLda(corpus=corpus, id2word=vocabs, num_topics=12, num_models=3)

# elda.print_topics()

# import re
# k = 1
# for i,j in elda.print_topics():
#     x = re.findall(r'"(.*?)"', str(j))
#     z = ' '.join(x)
#     print('Topic{}'.format(k),z)
#     k += 1

"""### 3.3 word embedding
1. word embedding with only emoji
2. word embedding with both emoji and words

#### 3.3.1 Word embedding model with only emojis

(1) All emojis
"""

# 3.3.1
####
# all emojis
emlist = [em.split() for em in df['emojis']]
emlist[:3]

posit = (['üòÇ', 'üòÖ'],['üòÖ', 'üëç'],['üôÉ', 'üòÖ'],['üí©', 'üò°'],['üòÄ','üëè'],['üéâ','üòÜ'],['üòò', '‚ô•'])
negat = (['üòÖ', 'üëç'],['üíã', '‚ô•'],['üòò', '‚ô•'])

# Commented out IPython magic to ensure Python compatibility.
from gensim.models import Word2Vec
#*************reported*********
# set model parameters
num_features = 60    # Word vector dimensionality, tested 10 to 100
min_word_count = 50    # Minimum word count, tested 10 to 100
context = 5           # Context window size tested 1 to 20
sg = 0                # skipgram=1, cbow=0  ###cbow shows more intuitive results, tested 0,1

# train the model
model_emoall = Word2Vec(emlist, vector_size=num_features, window=context, min_count=min_word_count, sg=sg)
# print(model_emoall.wv.key_to_index.keys())

list = allemoji
print(len(list))
#*************reported*********
# for i in list:
#     print('the most similar words to '+i+' are:')
#     print(model_emoall.wv.most_similar(i))
#     print('\n')

# analog task
#*************reported*********
# for i in posit:
#     print('{}+{}=?'.format(i[0],i[1]))
#     print(model_emoall.wv.most_similar(positive=i)[:3])
#     print()

# for i in negat:
#     print('{}-{}=?'.format(i[0],i[1]))
#     print(model_emoall.wv.most_similar(negative=i)[:3])
#     print()

# # only print answers
# for i in posit:
#     print(model_emoall.wv.most_similar(positive=i)[:3][:3])
#     print()
# print('----')
# for i in negat:
#     print(model_emoall.wv.most_similar(negative=i)[:3][:3])
#     print()

"""***(2) only concerned emojis***"""



#####
# only concerned emojis
emlist2 = cels

# Commented out IPython magic to ensure Python compatibility.

from gensim.models import Word2Vec
# set model parameters
num_features =30    # Word vector dimensionality
min_word_count = 5    # Minimum word count
context = 15           # Context window size
sg = 0                # skipgram=1, cbow=0


# train the model
model_emo = Word2Vec(emlist2, vector_size=num_features, window=context, min_count=min_word_count, sg=sg)

list = allemoji
print(list)
# for i in list:
#     print('the most similar words to '+i+' are:')
#     print(model_emo.wv.most_similar(i)[:3])
#     print('\n')

# # have fun
# # not very satisfying
# for i in (['üòÇ', 'üòÖ'],['üòÖ', 'üíñ'],['üò†', 'üòÖ'],['üòò', 'üíî'],['üòò', '‚ô•']):
#     print('{}+{}=?'.format(i[0],i[1]))
#     print(model_emo.wv.most_similar(positive=i)[:3])
#     print()

# for i in (['üòÖ', 'üò°'],['üòç', 'üíñ']):
#     print('{}-{}=?'.format(i[0],i[1]))
#     print(model_emo.wv.most_similar(negative=i)[:3])
#     print()

"""***(3). emojis+text***"""

##################
# 3.3.2 Word embedding model with emojis&text

# emojis+tweets
emlist = [em.split() for em in df['stweets']]
emlist[:3]

# Commented out IPython magic to ensure Python compatibility.
from gensim.models import Word2Vec
#*************reported*********
# set model parameters
num_features = 300    # Word vector dimensionality
min_word_count = 20    # Minimum word count-Ignores all words with total frequency lower than this
context = 15           # Context window size
sg = 3                # skipgram=3, cbow=0

# train the model
model_emotext = Word2Vec(emlist, vector_size=num_features, window=context, min_count=min_word_count, sg=sg)
# this set of parameters is good

## save the model
# you can save the trained model for future use
model_emotext.save("en_word2vec_skipgram_300.model")#ÂêéÁª≠ÂèØ‰ª•Áõ¥Êé•Âä†ËΩΩ‰∏çÁî®ÈáçÊñ∞Â≠¶

# you can load saved model later as follows:
# model_w2v = Word2Vec.load("en_word2vec_skipgram_300.model")

# save all word embeddings
all_word_vectors = model_emotext.wv.vectors
print(type(all_word_vectors))
print(all_word_vectors.shape) # num_words * num_features

all_word_vectors

from gensim.models import KeyedVectors
# Store just the words + their trained embeddings.
word_vectors = model_emotext.wv
word_vectors.save("en_word2vec.wordvectors")

# Load back with memory-mapping = read-only, shared across processes.
wv = KeyedVectors.load("en_word2vec.wordvectors", mmap='r')
# vector = wv['sun']  # Get numpy vector of a word
# print(vector)

#*************reported*********
list = allemoji
# for i in list:
#     print('the most similar words to '+i+' are:')
#     print(model_emotext.wv.most_similar(i,topn=30))
#     print('\n')

# save the most similar words of all unique emojis
import emoji

dic = {}
for i in uniquewordslist:
    try:
        ls = []
        count = 0
        simils = model_emotext.wv.most_similar(i,topn=100)
        for simi in simils:
            count += 1
            (k,v) = simi
            if k not in emoji.EMOJI_DATA.keys() and len(ls)<5:
                ls.append(k)# the first 5 text tokens for each concerned emoji
        emotext = " ".join(ls)
        dic[i] = dic.get(i,emotext)
    except:
        continue

print(dic)

df_name = pd.DataFrame.from_dict(dic,orient='index')
df_name

df_name.to_csv("en_most_similar_names.csv")

#*************reported*********
# have fun
# not very satisfying
# for i in posit:
#     print('{}+{}=?'.format(i[0],i[1]))
#     print(model_emotext.wv.most_similar(positive=i)[:10])
#     print()

# for i in negat:
#     print('{}-{}=?'.format(i[0],i[1]))
#     print(model_emotext.wv.most_similar(negative=i)[:10])
#     print()

# Commented out IPython magic to ensure Python compatibility.
####
## cbow
# cbow Âú®Ëã±ÊñáÊúâÊñáÂ≠óÁöÑÊÉÖÂÜµ‰∏ãÊïàÊûú‰∏çÊòØÂæàÂ•Ω
from gensim.models import Word2Vec
# set model parameters
num_features = 300    # Word vector dimensionality
min_word_count = 50    # Minimum word count-Ignores all words with total frequency lower than this
context = 20           # Context window size
sg = 0                # skipgram=3, cbow=0

# train the model
# %time model_emotext = Word2Vec(emlist, vector_size=num_features, window=context, min_count=min_word_count, sg=sg)

list = allemoji
# for i in list:
#     print('the most similar words to '+i+' are:')
#     print(model_emotext.wv.most_similar(i,topn=20))
#     print('\n')

# # have fun
# # not very satisfying
# for i in posit:
#     print('{}+{}=?'.format(i[0],i[1]))
#     print(model_emotext.wv.most_similar(positive=i))
#     print()

# for i in negat:
#     print('{}-{}=?'.format(i[0],i[1]))
#     print(model_emotext.wv.most_similar(negative=i))
#     print()



"""***emojis+text Visualization***"""

# visualization
keys = ['üòÄ','üòò','üò°','üòÇ','üòÖ']
# keys = allemoji
embedding_clusters = []
word_clusters = []
for word in keys:
    embeddings = []
    words = []
    try:
        for similar_word, _ in model_emotext.wv.most_similar(word, topn=50):
            if similar_word not in emoji.EMOJI_DATA.keys() and len(words)<15:
                words.append(similar_word)# most similar 10 text tokens
                embeddings.append(model_emotext.wv[similar_word])
        embedding_clusters.append(embeddings)
        word_clusters.append(words)
    except:
        continue

from sklearn.manifold import TSNE
import numpy as np

embedding_clusters = np.array(embedding_clusters)
n, m, k = embedding_clusters.shape
tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)
embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import matplotlib.cm as cm
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore') #ignore the warning
# plt.rcParams['font.family'] = prop.get_family()

def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):
    plt.figure(figsize=(20, 9))
    colors = cm.rainbow(np.linspace(0, 1, len(labels)))
    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):
        x = embeddings[:, 0]
        y = embeddings[:, 1]
        plt.scatter(x, y, c=color, alpha=a, label=label)
        for i, word in enumerate(words):
            plt.annotate(word, alpha=1, xy=(x[i], y[i]), xytext=(5, 2),
                         textcoords='offset points', ha='right', va='bottom', size=13)
    plt.legend(loc=4,prop={'size': 20})
    plt.title(title)
    plt.grid(True)
    if filename:
        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')
    plt.show()


tsne_plot_similar_words('Similar words in English Tweets', keys, embeddings_en_2d, word_clusters, 1,
                        'similar_words.png')